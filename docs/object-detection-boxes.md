---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Object detection boxes

> *You shouldn't set up limits in boundless openness,
> but if you set up limitlessness as boundless openness,
> you've trapped yourself*

Boxes are quick to draw âœï¸, but finicky to train a neural network with.
This time, we'll show you a geospatial object detection ğŸ•µï¸ problem, where the
objects are defined by a bounding box ğŸ”² with a specific class.
By the end of this lesson, you should be able to:

- Read OGR supported vector files and obtain the bounding boxes ğŸŸ¨ of each
  geometry
- Convert bounding boxes from geographic coordinates to ğŸ–¼ï¸ image coordinates
  while clipping to the image extent
- Use an affine transform to convert boxes in image coordinates to ğŸŒ
  geographic coordinates

ğŸ”— Links:
- https://planetarycomputer.microsoft.com/dataset/ms-buildings#Example-Notebook
- https://github.com/microsoft/GlobalMLBuildingFootprints/
- https://mlhub.earth/datasets?tags=object+detection

## ğŸ‰ **Getting started**

These are the tools ğŸ› ï¸ you'll need.

```{code-cell}
import contextily
import numpy as np
import geopandas as gpd
import matplotlib.patches
import matplotlib.pyplot as plt
import pandas as pd
import planetary_computer
import pystac_client
import rioxarray
import torchdata
import xarray as xr
import zen3geo
```

## 0ï¸âƒ£ Find high-resolution imagery and building footprints ğŸŒ‡

Let's take a look at buildings over
[Kampong Ayer](https://en.wikipedia.org/wiki/Kampong_Ayer), Brunei ğŸ‡§ğŸ‡³! We'll
use {py:func}`contextily.bounds2img` to get some 4-band RGBA
ğŸŒˆ [optical imagery](https://www.arcgis.com/home/item.html?id=10df2279f9684e4a9f6a7f08febac2a9)
in a {py:class}`numpy.ndarray` format.

```{code-cell}
image, extent = contextily.bounds2img(
    w=114.94,
    s=4.88,
    e=114.95,
    n=4.89,
    ll=True,
    source=contextily.providers.Esri.WorldImagery,
)
print(f"Spatial extent in EPSG:3857: {extent}")
print(f"Image dimensions (height, width, channels): {image.shape}")
```

This is how Brunei's ğŸš£ Venice of the East looks like from above.

```{code-cell}
fig, ax = plt.subplots(nrows=1, figsize=(9, 9))
plt.imshow(X=image, extent=extent)
```

```{tip}
For more raster basemaps, check out:
- https://xyzservices.readthedocs.io/en/stable/introduction.html#overview-of-built-in-providers
- https://leaflet-extras.github.io/leaflet-providers/preview/
```

### Georeference image using rioxarray ğŸŒ

To enable slicing ğŸ”ª with xbatcher later, we'll need to turn the
{py:class}`numpy.ndarray` image ğŸ–¼ï¸ into an {py:class}`xarray.DataArray` grid
with coordinates ğŸ–¼ï¸. If you already have a georeferenced grid (e.g. from
{py:class}`zen3geo.datapipes.RioXarrayReader`), this step can be skipped â­ï¸.


```{code-cell}
# Turn RGBA image from channel-last to channel-first and get 3-band RGB only
_image = image.transpose(2, 0, 1)  # Change image from (H, W, C) to (C, H, W)
rgb_image = _image[0:3, :, :]  # Get just RGB by dropping RGBA's alpha channel
print(f"RGB image shape: {rgb_image.shape}")
```

Georeferencing is done by putting the ğŸš¦ RGB image into an
{py:class}`xarray.DataArray` object with (band, y, x) coordinates, and then
setting a coordinate reference system ğŸ“ using
{py:meth}`rioxarray.rioxarray.XRasterBase.set_crs`.

```{code-cell}
left, right, bottom, top = extent  # xmin, xmax, ymin, ymax
dataarray = xr.DataArray(
    data=rgb_image,
    coords=dict(
        band=[0, 1, 2],  # Red, Green, Blue
        y=np.linspace(start=top, stop=bottom, num=rgb_image.shape[1]),
        x=np.linspace(start=left, stop=right, num=rgb_image.shape[2]),
    ),
    dims=("band", "y", "x"),
)
dataarray = dataarray.rio.write_crs(input_crs="EPSG:3857")
dataarray
```

### Load cloud-native vector files ğŸ’ 

Now to pull in some building footprints ğŸ›–. Let's make a STAC API query to get
a [GeoParquet](https://github.com/opengeospatial/geoparquet) file (a
cloud-native columnar ğŸ€¤ geospatial vector file format) over our study area.

```{code-cell}
catalog = pystac_client.Client.open(
    url="https://planetarycomputer.microsoft.com/api/stac/v1"
)
items = catalog.search(
    collections=["ms-buildings"], query={"msbuildings:region": {"eq": "Brunei"}}
)
item = next(items.get_items())
item
```

Next, we'll sign ğŸ” the URL to the STAC Item Asset, and load â¤µï¸ the GeoParquet
file using {py:func}`geopandas.read_parquet`.

```{code-cell}
asset = planetary_computer.sign(item.assets["data"])

geodataframe = gpd.read_parquet(
    path=asset.href, storage_options=asset.extra_fields["table:storage_options"]
)
geodataframe
```

This {py:class}`geopandas.GeoDataFrame` contains building outlines across
Brunei ğŸ‡§ğŸ‡³. Let's do a spatial subset âœ‚ï¸ to just the Kampong Ayer study area
using {py:attr}`geopandas.GeoDataFrame.cx`, and reproject it using
{py:meth}`geopandas.GeoDataFrame.to_crs` to match the coordinate reference
system of the optical image.

```{code-cell}
_gdf_kpgayer = geodataframe.cx[114.94:114.95, 4.88:4.89]
gdf_kpgayer = _gdf_kpgayer.to_crs(crs="EPSG:3857")
gdf_kpgayer
```

Preview ğŸ‘€ the building footprints to check that things are in the right place.

```{code-cell}
ax = gdf_kpgayer.plot(figsize=(9, 9))
contextily.add_basemap(
    ax=ax,
    source=contextily.providers.Stamen.TonerLite,
    crs=gdf_kpgayer.crs.to_string(),
)
ax
```

Hmm, seems like the Stamen basemap doesn't know that some of the buildings are
on water ğŸ˜‚.


## 1ï¸âƒ£ Pair image chips with bounding boxes ğŸ§‘â€ğŸ¤â€ğŸ§‘

Here comes the fun ğŸ› part! This section is all about generating 128x128 chips
ğŸ«¶ paired with bounding boxes. Let's go ğŸš²!

### Create 128x128 raster chips and clip vector geometries with it âœ‚ï¸

From the large 1280x1280 scene ğŸ–½ï¸, we will first slice out a hundred 128x128
chips ğŸ• using {py:class}`zen3geo.datapipes.XbatcherSlicer` (functional name:
`slice_with_xbatcher`).

```{code-cell}
dp_raster = torchdata.datapipes.iter.IterableWrapper(iterable=[dataarray])
dp_xbatcher = dp_raster.slice_with_xbatcher(input_dims={"y": 128, "x": 128})
dp_xbatcher
```

For each 128x128 chip ğŸ•, we'll then find the vector geometries ğŸŒ™ that fit
within the chip's spatial extent. This will be ğŸ¤¸ done using
{py:class}`zen3geo.datapipes.GeoPandasRectangleClipper` (functional name:
`clip_vector_with_rectangle`).

```{code-cell}
dp_vector = torchdata.datapipes.iter.IterableWrapper(iterable=[gdf_kpgayer])
dp_clipped = dp_vector.clip_vector_with_rectangle(mask_datapipe=dp_xbatcher)
dp_clipped
```

```{important}
When using {py:class}`zen3geo.datapipes.GeoPandasRectangleClipper` ğŸ’‡, there
should only be one 'global' ğŸŒ vector {py:class}`geopandas.GeoSeries` or
{py:class}`geopandas.GeoDataFrame`.

If your raster DataPipe has chips ğŸ• with different coordinate reference
systems (e.g. multiple UTM Zones ğŸŒğŸŒğŸŒ),
{py:class}`zen3geo.datapipes.GeoPandasRectangleClipper` will actually reproject
ğŸ”„ the 'global' vector to the coordinate reference system of each chip, and
clip âœ‚ï¸ the geometries accordingly to the chip's bounding box extent ğŸ˜.
```

This ``dp_clipped`` DataPipe will yield ğŸ¤² a tuple of ``(vector, raster)``
objects for each 128x128 chip. Let's inspect ğŸ§ one to see how they look like.

```{code-cell}
# Get one chip with over 10 building footprint geometries
for vector, raster in dp_clipped:
    if len(vector) > 10:
        break
```

These are the spatially subsetted vector geometries ğŸŒ™ in one 128x128 chip.

```{code-cell}
vector
```

This is the raster chip/mask ğŸ¤¿ used to clip the vector.

```{code-cell}
raster
```

And here's a side by side visualization of the ğŸŒˆ RGB chip image (left) and
ğŸ”· vector building footprint polygons (right).

```{code-cell}
fig, ax = plt.subplots(ncols=2, figsize=(18, 9), sharex=True, sharey=True)
raster.__xarray_dataarray_variable__.plot.imshow(ax=ax[0])
vector.plot(ax=ax[1])
```

Cool, these buildings are part of the ğŸ¬
[Yayasan Shopping Complex](https://web.archive.org/web/20220906020248/http://www.yayasancomplex.com)
in Bandar Seri Begawan ğŸŒ†. We can see that the raster image ğŸ–¼ï¸ on the left
aligns ok with the vector polygons ğŸ’  on the right.

```{note}
The optical ğŸ›°ï¸ imagery shown here is **not** the imagery used to digitize the
[building footprints](https://planetarycomputer.microsoft.com/dataset/ms-buildings)
ğŸ¢! This is an example tutorial using two different data sources, that we just
so happened to have plotted in the same geographic space ğŸ˜.
```

### From polygons in geographic coordinates to boxes in image coordinates â†•ï¸

Up to this point, we still have the actual ğŸ›– building footprint polygons. In
this step ğŸ“¶, we'll convert these polygons into a format suitable for 'basic'
object detection ğŸ¥… models in computer vision. Specifically:

1. The polygons ğŸŒ™ (with multiple vertices) will be simplified to a horizontal
   bounding box ğŸ”² with 4 corner vertices only.
2. The ğŸŒ geographic coordinates of the box which use lower left corner and
   upper right corner (i.e. y increases from South to North â¬†ï¸) will be
   converted to ğŸ–¼ï¸ image coordinates (0-128) which use the top left corner and
   bottom right corner (i.e y increases from Top to Bottom â¬‡ï¸).

Let's start by using {py:attr}`geopandas.GeoSeries.bounds` to get the
geographic bounds ğŸ—ºï¸ of each building footprint geometry ğŸ“ in each 128x128
chip.

```{code-cell}
def polygon_to_bbox(geom_and_chip) -> (gpd.GeoDataFrame, xr.Dataset):
    """
    Get bounding box (minx, miny, maxx, maxy) coordinates for each geometry in
    a geopandas.GeoDataFrame.

                          (maxx,maxy)
               ul-------ur
             ^  |       |
             |  |  geo  |    y increases going up, x increases going right
             y  |       |
               ll-------lr
    (minx,miny)    x-->

    """
    gdf, chip = geom_and_chip
    bounds: gpd.GeoDataFrame = gdf.bounds
    assert tuple(bounds.columns) == ("minx", "miny", "maxx", "maxy")

    return bounds, chip
```

```{code-cell}
dp_bbox = dp_clipped.map(fn=polygon_to_bbox)
```

Next, the geographic ğŸ—ºï¸ bounding box coordinates (in EPSG:3857) will be
converted to image ğŸ–¼ï¸ or pixel coordinates (0-128 scale). The y-direction will
be flipped ğŸ¤¸ upside down, and we'll be using the spatial bounds (or corner
coordinates) of the 128x128 image chip as a reference ğŸ“.

```{code-cell}
def geobox_to_imgbox(bbox_and_chip) -> (pd.DataFrame, xr.Dataset):
    """
    Convert bounding boxes in a pandas.DataFrame from geographic coordinates
    (minx, miny, maxx, maxy) to image coordinates (x1, y1, x2, y2) based on the
    spatial extent of a raster image chip.

        (x1,y1)
               ul-------ur
             y  |       |
             |  |  img  |    y increases going down, x increases going right
             v  |       |
               ll-------lr
                   x-->    (x2,y2)

    """
    geobox, chip = bbox_and_chip

    x_res, y_res = chip.rio.resolution()
    assert y_res < 0

    left, bottom, right, top = chip.rio.bounds()
    assert top > bottom

    imgbox = pd.DataFrame()
    imgbox["x1"] = (geobox.minx - left) / x_res  # left
    imgbox["y1"] = (top - geobox.maxy) / -y_res  # top
    imgbox["x2"] = (geobox.maxx - left) / x_res  # right
    imgbox["y2"] = (top - geobox.miny) / -y_res  # bottom

    assert all(imgbox.x2 > imgbox.x1)
    assert all(imgbox.y2 > imgbox.y1)

    return imgbox, chip
```

```{code-cell}
dp_ibox = dp_bbox.map(fn=geobox_to_imgbox)
```

Now to plot ğŸ¨ and double check that the boxes are positioned correctly in
image space ğŸŒŒ.

```{code-cell}
# Get one chip with over 10 building footprint geometries
for ibox, ichip in dp_ibox:
    if len(ibox) > 10:
        break
```

```{code-cell}
fig, ax = plt.subplots(ncols=2, figsize=(18, 9), sharex=True, sharey=True)
ax[0].imshow(X=ichip.__xarray_dataarray_variable__.transpose("y", "x", "band"))
for i, row in ibox.iterrows():
    rectangle = matplotlib.patches.Rectangle(
        xy=(row.x1, row.y1),
        width=row.x2 - row.x1,
        height=row.y2 - row.y1,
        edgecolor="blue",
        linewidth=1,
        facecolor="none",
    )
    ax[1].add_patch(rectangle)
```

Cool, the ğŸŸ¦ bounding boxes on the right subplot are correctly positioned ğŸ§­
(compare it with the figure in the previous subsection).

```{hint}
Instead of a bounding box ğŸ¥¡ object detection task, you can also use the
building polygons ğŸ˜ï¸ for a segmentation task ğŸ§‘â€ğŸ¨ following
{doc}`./vector-segmentation-masks`.

If you still prefer doing object detection ğŸ•µï¸, but want a different box format
(see options in {py:func}`torchvision.ops.box_convert`),
like ğŸŒ centre-based coordinates with width and height (`cxcywh`), or
ğŸ“¨ oriented/rotated bounding box coordinates, feel free to implement your own
function and DataPipe for it ğŸ¤—!
```
