---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# Vector segmentation labels

> *Clouds float by, water flows on;
> in movement there is no grasping, in Chan there is no settling*

For supervised machine learning, labels are needed in addition to the input
image. Here, we'll step through an example workflow on matching vector label
data (points, lines, polygons) to Earth Observation data inputs. Specifically,
this tutorial will cover:

- Reading shapefiles directly from the web via pyogrio
- Rasterizing vector polygons from a geopandas.GeoDataFrame to an xarray.DataArray
- Pairing satellite images with the rasterized labels followed by chipping


## ğŸ‰ **Getting started**

These are the tools ğŸ› ï¸ you'll need.

```{code-cell}
import matplotlib.pyplot as plt
import numpy as np
import planetary_computer
import pyogrio
import pystac
import torch
import torchdata
import xarray as xr
import zen3geo
```

## 0ï¸âƒ£ Find cloud-hosted raster and vector data â›³

In this case study, we'll look at the flood water extent over Johor,
Malaysia ğŸ‡²ğŸ‡¾ on 15 Dec 2019 that were digitized by UNITAR-UNOSAT's rapid mapping
service over Synthetic Aperture Radar (SAR) images. Specifically, we'll be
using the Sentinel-1 Ground Range Detected (GRD) product's VV polarization
channel.

ğŸ”— Links:
- https://www.unitar.org/maps/unosat-rapid-mapping-service
- https://unitar.org/maps/countries
- [Microsoft Planetary Computer STAC Explorer](https://planetarycomputer.microsoft.com/explore?c=103.6637%2C2.1494&z=8.49&v=2&d=sentinel-1-grd&s=false%3A%3A100%3A%3Atrue&ae=0&m=cql%3Afc3d85b6ab43d3e8ebe168da0206f2cf&r=VV%2C+VH+False-color+composite)

```{code-cell}
item_url = "https://planetarycomputer.microsoft.com/api/stac/v1/collections/sentinel-1-grd/items/S1A_IW_GRDH_1SDV_20191215T224757_20191215T224822_030365_037955"

# Load the individual item metadata and sign the assets
item = pystac.Item.from_file(item_url)
signed_item = planetary_computer.sign(item)
signed_item
```

This is how the Sentinel-1 image looks like over Johor in Peninsular Malaysia
on 15 Dec 2019.

![Sentinel-1 image over Johor, Malaysia on 20191215](https://planetarycomputer.microsoft.com/api/data/v1/item/preview.png?collection=sentinel-1-grd&item=S1A_IW_GRDH_1SDV_20191215T224757_20191215T224822_030365_037955&assets=vv&assets=vh&expression=vv%2Cvh%2Cvv%2Fvh&rescale=0%2C500&rescale=0%2C300&rescale=0%2C7&tile_format=png)

### Load and reproject image data ğŸ”„

To keep things simple, we'll load just the VV channel into a DataPipe via
{py:class}`zen3geo.datapipes.rioxarray.RioXarrayReaderIterDataPipe`

```{code-cell}
url = signed_item.assets["vv"].href
dp = torchdata.datapipes.iter.IterableWrapper(iterable=[url])
# Reading lower resolution grid using overview_level=3
dp_rioxarray = dp.read_from_rioxarray(overview_level=3)
dp_rioxarray
```

The Sentinel-1 image from Planetary Computer comes in longitude/latitude
geographic coordinates by default (EPSG:4326). To make the pixels more equal
area, we can project it to a local projected coordinate system instead.

```{code-cell}
def reproject_to_local_utm(dataarray: xr.DataArray, resolution: float=100.0) -> xr.DataArray:
    """
    Reproject an xarray.DataArray grid from EPSG;4326 to a local UTM coordinate
    reference system.
    """
    # Estimate UTM coordinate reference from a single pixel
    pixel = dataarray.isel(y=slice(0, 1), x=slice(0,1))
    new_crs = dataarray.rio.reproject(dst_crs="EPSG:4326").rio.estimate_utm_crs()

    return dataarray.rio.reproject(dst_crs=new_crs, resolution=resolution)
```

```{code-cell}
dp_reprojected = dp_rioxarray.map(fn=reproject_to_local_utm)
```

```{note}
Note that Universal Transverse Mercator (UTM) isn't an equal-area projection
system. However, Sentinel-1 satellite scenes from Copernicus are usually
distributed in a UTM coordinate reference system, and UTM is typically a close
enough approximation to the local geographic area, or at least it won't matter
much when we're looking at spatial resolutions over several 10s of metres.
```

### Transform and visualize raster data ğŸ”

Let's visualize the Sentinel-1 image, but before that, we'll transform the VV
data from linear to [decibel](https://en.wikipedia.org/wiki/Decibel) units.

```{code-cell}
def linear_to_decibel(dataarray: xr.DataArray) -> xr.DataArray:
    """
    Transforming the input xarray.DataArray's VV or VH values from linear to
    decibel scale using the formula ``10 * log_10(x)``.
    """
    # Mask out areas with 0 so that np.log10 is not undefined
    da_linear = dataarray.where(cond=dataarray != 0)
    da_decibel = 10 * np.log10(da_linear)
    return da_decibel
```

```{code-cell}
dp_decibel = dp_reprojected.map(fn=linear_to_decibel)
dp_decibel
```

Now to visualize the transformed Sentinel-1 image ğŸ–¼ï¸. Let's zoom in ğŸ”­ to one
of the analysis extent areas we'll be working on later.

```{code-cell}
it = iter(dp_decibel)
dataarray = next(it)

da_clip = dataarray.rio.clip_box(minx=371483, miny=190459, maxx=409684, maxy=229474)
da_clip.isel(band=0).plot.imshow(figsize=(11.5, 9), cmap="Blues_r", vmin=18, vmax=26)
```

Notice how the darker blue areas tend to correlate more with water features
like the meandering rivers and the sea on the NorthEast. This is because the
SAR signal which is side looking reflects off flat water bodies like a mirror,
with little energy getting reflected back directly to the sensor (hence why it
looks darker).

### Load and visualize cloud-hosted vector files ğŸ’ 

Let's now load some vector data from the web ğŸ•¸ï¸. These are polygons of the
segmented ğŸŒŠ water extent digitized by UNOSAT's AI Based Rapid Mapping Service.
We'll be converting these vector polygons to ğŸŒˆ raster masks later.

ğŸ”— Links:
- https://github.com/UNITAR-UNOSAT/UNOSAT-AI-Based-Rapid-Mapping-Service
- [Humanitarian Data Exchange link to polygon dataset](https://data.humdata.org/dataset/waters-extents-as-of-15-december-2019-over-kota-tinggi-and-mersing-district-johor-state-of)
- [Disaster Risk Monitoring Using Satellite Imagery online course](https://courses.nvidia.com/courses/course-v1:DLI+S-ES-01+V1)

```{code-cell}
# https://gdal.org/user/virtual_file_systems.html#vsizip-zip-archives
shape_url = "/vsizip/vsicurl/https://unosat-maps.web.cern.ch/MY/FL20191217MYS/FL20191217MYS_SHP.zip/ST1_20191215_WaterExtent_Johor_AOI2.shp"
```

This is a shapefile containing ğŸ”· polygons of the mapped water extent. Let's
put it into a DataPipe called {py:class}`zen3geo.datapipes.PyogrioReader`
(functional name: ``read_from_pyogrio``).

```{code-cell}
dp_shapes = torchdata.datapipes.iter.IterableWrapper(iterable=[shape_url])
dp_pyogrio = dp_shapes.read_from_pyogrio()
```

This will take care of loading the shapefile into a
{py:class}`geopandas.GeoDataFrame` object. Let's take a look at the data table
to see what attributes are inside.

```{code-cell}
it = iter(dp_pyogrio)
geodataframe = next(it)
geodataframe.dropna(axis="columns")
```

Cool, and we can also visualize the polygons ğŸ”· on a 2D map. To align the
coordinates with the ğŸ›°ï¸ Sentinel-1 image above, we'll first use
{py:meth}`geopandas.GeoDataFrame.to_crs` to reproject the vector from ğŸŒ
EPSG:4326 (longitude/latitude) to ğŸŒ EPSG:32648 (UTM Zone 48N).

```{code-cell}
print(f"Original bounds in EPSG:4326:\n{geodataframe.bounds}")
gdf = geodataframe.to_crs(crs="EPSG:32648")
print(f"New bounds in EPSG:32648:\n{gdf.bounds}")
```

Plot it with {py:meth}`geopandas.GeoDataFrame.plot`. This vector map ğŸ—ºï¸ should
correspond to the zoomed in Sentinel-1 image plotted earlier above.

```{code-cell}
gdf.plot(figsize=(11.5, 9))
```

```{tip}  
Make sure to understand your raster and vector datasets well first! Open the
files up in your favourite ğŸŒ Geographic Information System (GIS) tool, see how
they actually look like spatially. Then you'll have a better idea to decide on
how to create your data pipeline. The zen3geo way puts you as the Master ğŸ§™ in
control.
```


## 1ï¸âƒ£ Create a canvas to paint on ğŸ¨

In this section, we'll work on converting the flood water ğŸŒŠ polygons above
from a ğŸš© vector to a ğŸŒˆ raster format, i.e. rasterization. This will be done
in two steps ğŸ“¶:

1. Defining a blank canvas ğŸï¸
2. Paint the polygons onto this blank canvas ğŸ§‘â€ğŸ¨

For this, we'll be using tools from {py:meth}`zen3geo.datapipes.datashader`.
Let's see how this can be done.

### Blank canvas from template raster ğŸ–¼ï¸

A canvas represents a 2D area with a height and a width ğŸ“. For us, we'll be
using a {py:class}`datashader.Canvas`, which also defines the range of y-values
(ymin to ymax) and x-values (xmin to xmax), essentially coordinates for
every unit ğŸ‡¾ height and ğŸ‡½ width.

Since we already have a Sentinel-1 ğŸ›°ï¸ raster grid with defined height/width
and y/x coordinates, let's use it as a ğŸ“„ template to define our canvas. This
is done via {py:class}`zen3geo.datapipes.XarrayCanvas` (functional name:
``canvas_from_xarray``).

```{code-cell}
dp_canvas = dp_decibel.canvas_from_xarray()
dp_canvas
```

Cool, and here's a quick inspection ğŸ‘€ of the canvas dimensions and metadata.

```{code-cell}
it = iter(dp_canvas)
canvas = next(it)
print(f"Canvas height: {canvas.plot_height}, width: {canvas.plot_width}")
print(f"Y-range: {canvas.y_range}")
print(f"X-range: {canvas.x_range}")
print(f"Coordinate reference system: {canvas.crs}")
```

This information should match the template the Sentinel-1 dataarray ğŸ.

```{code-cell}
print(f"Dimensions: {dict(dataarray.sizes)}")
print(f"Affine transform: {dataarray.rio.transform()}")
print(f"Bounding box: {dataarray.rio.bounds()}")
print(f"Coordinate reference system: {dataarray.rio.crs}")
```

### Rasterize vector polygons onto canvas ğŸ–Œï¸

Now's the time to paint or rasterize the
vector {py:class}`geopandas.GeoDataFrame` polygons ğŸ”· onto the blank
{py:class}`zen3geo.datapipes.XarrayCanvas`! This would enable us to have a
direct pixel-wise X -> Y mapping â†”ï¸ between the Sentinel-1 image (X) and target
flood label (Y).

The vector polygons can be rasterized or painted ğŸ–Œï¸ onto the template canvas
using {py:class}`zen3geo.datapipes.DatashaderRasterizer` (functional name:
``rasterize_with_datashader``).

```{code-cell}
dp_datashader = dp_canvas.rasterize_with_datashader(vector_datapipe=dp_pyogrio)
dp_datashader
```

This will turn the vector {py:class}`geopandas.GeoDataFrame` into a
raster {py:class}`xarray.DataArray` grid, with the spatial coordinates and
bounds matching exactly with the template Sentinel-1 image ğŸ˜.

```{note}
Since we have just one Sentinel-1 ğŸ›°ï¸ image and one raster ğŸ’§ flood
mask, we have an easy 1:1 mapping. There are two other scenarios supported by
{py:class}`zen3geo.datapipes.DatashaderRasterizer`:

1. N:1 - Many {py:class}`datashader.Canvas` objects to one vector
   {py:class}`geopandas.GeoDataFrame`. The single vector geodataframe will be
   broadcasted to match the length of the canvas list. This is useful for
   situations when you have a ğŸŒ 'global' vector database that you want to pair
   with multiple ğŸ›°ï¸ satellite images.
2. N:N - Many {py:class}`datashader.Canvas` objects to many vector
   {py:class}`geopandas.GeoDataFrame` objects. In this case, the list of grids
   **must** â— have the same length as the list of vector geodataframes. E.g.
   if you have 5 grids, there must also be 5 vector files. This is so that a
   1:1 pairing can be done, useful when each raster tile ğŸ–½ has its own
   associated vector annotation.
```

```{seealso}
For more details on how rasterization of polygons work behind the scenes ğŸ¦,
check out {doc}`Datashader <datashader:index>`'s documentation on:

- {doc}`The datashader pipeline <getting_started/Pipeline>` (especially the
  section on Aggregation).
- {doc}`Rendering large collections of polygons <user_guide/Polygons>`
```


## 2ï¸âƒ£ Stack, slice and split ğŸ¥ª

At this point, we've got two datapipes that should be ğŸ§‘â€ğŸ¤â€ğŸ§‘ paired up in an X -> Y
manner:

1. The pre-processed Sentinel-1 ğŸŒˆ raster image in ``dp_decibel``
2. The rasterized ğŸ’§ flood segmentation masks in ``dp_datashader``

One way to get these two pieces in a Machine Learning ready chip format is via
a stack, slice and split â„¢ï¸ approach. Think of it like a sandwich, we first
stack the bread ğŸ and lettuce ğŸ¥¬, and then slice the pieces ğŸ• through the
layers once. Ok, that was a bad analogy, let's just stick with tensors ğŸ¤ª.

### Stacking the raster layers ğŸ¥

Each of our ğŸŒˆ raster inputs are {py:class}`xarray.DataArray` objects with the
same spatial resolution and extent ğŸªŸ, so these can be stacked into an
{py:class}`xarray.Dataset` with multiple data variables. First, we'll zip ğŸ¤
the two datapipes together using {py:class}`torchdata.datapipes.iter.Zipper`
(functional name: ``zip``)

```{code-cell}
dp_zip = dp_decibel.zip(dp_datashader)
dp_zip
```

This will result in a DataPipe where each item is a tuple of (X, Y) pairs ğŸ§‘â€ğŸ¤â€ğŸ§‘.
Just to illustrate what we've done so far, we can use
{py:class}`torchdata.datapipes.utils.to_graph` to visualize the data pipeline
â›“ï¸.

```{code-cell}
torchdata.datapipes.utils.to_graph(dp=dp_zip)
```

Next, let's combine ğŸ–‡ï¸ the two (X, Y) {py:class}`xarray.DataArray` objects in
the tuple into an {py:class}`xarray.Dataset` using
{py:class}`torchdata.datapipes.iter.Collator`. We'll also âœ‚ï¸ clip the dataset to
a bounding box area where the target water mask has no 0 or NaN values.

```{code-cell}
def xr_collate_fn(image_and_mask: tuple) -> xr.Dataset:
    """
    Combine a pair of xarray.DataArray (image, mask) inputs into an
    xarray.Dataset with two data variables named 'image' and 'mask'.
    """
    # Turn 2 xr.DataArray objects into 1 xr.Dataset with multiple data vars
    image, mask = image_and_mask
    dataset: xr.Dataset = image.isel(band=0).to_dataset(name="image")
    dataset["mask"] = mask

    # Clip dataset to bounding box extent of where labels are
    mask_extent: tuple = mask.where(cond=mask == 1, drop=True).rio.bounds()
    clipped_dataset: xr.Dataset = dataset.rio.clip_box(*mask_extent)

    return clipped_dataset
```

```{code-cell}
dp_dataset = dp_zip.collate(collate_fn=xr_collate_fn)
dp_dataset
```

Double check to see that resulting {py:class}`xarray.Dataset`'s image and mask
looks ok ğŸ™†â€â™‚ï¸.

```{code-cell}
it = iter(dp_dataset)
dataset = next(it)

# Create subplot with VV image on the left and Water mask on the right
fig, axs = plt.subplots(ncols=2, figsize=(11.5, 4.5), sharey=True)
dataset.image.plot.imshow(ax=axs[0], cmap="Blues_r")
axs[0].set_title("Sentinel-1 VV channel")
dataset.mask.plot.imshow(ax=axs[1], cmap="Blues")
axs[1].set_title("Water mask")
plt.show()
```
